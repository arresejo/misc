{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arresejo/misc/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfkNIPuWkZA7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from keras.models import Sequential, load_model, clone_model\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAABNpcOkdAC",
        "outputId": "c4f192cb-23cc-49f2-a823-15185cf0635e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC_c4xpakj6V"
      },
      "outputs": [],
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "env = gym.make(env_name).unwrapped\n",
        "\n",
        "#env.spec.max_episode_steps = 3000\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCs9YKcfUl1V",
        "outputId": "b966a8c1-93d5-492e-e8ae-2844f68800d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Apr 14 18:25:09 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    34W / 250W |    451MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU5OWVi-UsX8",
        "outputId": "d982fadb-7f98-410b-9c2f-fe7062c4d753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DxbfBwpkkfk"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, batch_size, gamma, alpha, epsilon, epsilon_decay, epsilon_min, update_frequency, memory_size):\n",
        "        self.env = env\n",
        "        if not self.env.continuous:  # discrete\n",
        "            self.num_actions = 4  # line 170 in source code: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
        "        else:\n",
        "            return f\"you still did not work on coninuous...\"\n",
        "        self.memory_size = memory_size\n",
        "        memory_space = 8 + 1 + 1 + 8 + 1  # states, action, reward, next_state, terminal   # TDL (soft-code it from the env)\n",
        "        self.memory_counter = 0\n",
        "        #self.memory = np.zeros((memory_space, self.memory_size))\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        # self.num_episodes = num_episodes  # in the train method\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.update_frequency = update_frequency\n",
        "\n",
        "    def agent_model(self, layer_list, input_dims, lr, loss, optimizer):  # fcn\n",
        "        model = Sequential()\n",
        "        for l in layer_list:\n",
        "            model.add(l)\n",
        "        model.compile(optimizer=optimizer(learning_rate=lr), loss=loss)\n",
        "        return model\n",
        "\n",
        "    def store_in_memory(self, sarst_list):  # sarst_list := (state,action,next_state,reward,terminal)\n",
        "        # store transition (S, A, R, S', terminal) in replay memory D\n",
        "        self.memory[:, self.memory_counter % self.memory_size] = np.concatenate(\n",
        "            tuple(np.array(x, dtype=float, ndmin=1) for x in sarst_list))\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            action = np.argmax(self.model_q1.predict(state.reshape((1, 8))))  # epsilon-greedy\n",
        "        else:\n",
        "            action = np.random.randint(self.num_actions)\n",
        "        return action\n",
        "\n",
        "    def get_rand_sample(self):\n",
        "        batch_indexes = np.random.choice(len(np.array(self.memory)), self.batch_size)\n",
        "\n",
        "        states, actions, next_states, rewards, terminals = ([] for _ in range(5))\n",
        "\n",
        "        for index in batch_indexes:\n",
        "            states.append(self.memory[index][0])\n",
        "            actions.append(self.memory[index][1])\n",
        "            next_states.append(self.memory[index][2])\n",
        "            rewards.append(self.memory[index][3])\n",
        "            terminals.append(self.memory[index][4])\n",
        "\n",
        "        return np.array(states).squeeze(), \\\n",
        "               np.array(actions).squeeze(), \\\n",
        "               np.array(next_states), \\\n",
        "               np.array(rewards), \\\n",
        "               np.array(terminals)\n",
        "\n",
        "    def train(self, num_episodes, layer_list, input_dims=8, loss='mse',\n",
        "              optimizer=Adam):  # TDL: default of input_dims as num of states\n",
        "        self.model_q1 = self.agent_model(layer_list, input_dims, self.alpha, loss, optimizer)\n",
        "        #self.model_q2 = clone_model(self.model_q1)\n",
        "        #self.model_q2.set_weights(self.model_q1.get_weights())\n",
        "        reward_steps_hist = []  # tupple per episode:  (reward, num_steps)\n",
        "        scores  = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "            terminal = False\n",
        "            state = self.env.reset()  # seed=seed\n",
        "\n",
        "            score = 0\n",
        "\n",
        "            #while not terminal:\n",
        "            for _ in range(3000):\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, terminal, _ = self.env.step(action)\n",
        "\n",
        "                self.memory.append((state, action, next_state, reward, terminal))\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if len(self.memory) >= self.batch_size:\n",
        "\n",
        "                    states, actions, next_states, rewards, terminals = self.get_rand_sample()\n",
        "\n",
        "                    q_next_state = self.model_q1.predict_on_batch(next_states)\n",
        "                    q_target = self.model_q1.predict_on_batch(states)\n",
        "\n",
        "                    #q_target[np.arange(self.batch_size), actions] = rewards + (terminals == 0) * self.gamma * np.amax(q_next_state, axis=1)\n",
        "                    q_target[np.arange(self.batch_size), actions] = rewards + self.gamma * np.amax(q_next_state, axis=1) *  (1 - terminals)\n",
        "\n",
        "                    self.model_q1.fit(states, q_target, verbose=0)\n",
        "\n",
        "                    self.epsilon = np.max(((self.epsilon * self.epsilon_decay), self.epsilon_min))\n",
        "\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                steps += 1\n",
        "\n",
        "                if terminal:\n",
        "                  scores.append(score)\n",
        "                  print(\"Episode = {}, Score = {}, Avg_Score = {}\".format(episode, score, np.mean(scores[-100:])))\n",
        "                  print((episode, total_reward, steps, self.epsilon))\n",
        "                  break\n",
        "\n",
        "            print(f\"End of episode {episode}\")\n",
        "\n",
        "            #print((episode, total_reward, steps, self.epsilon))\n",
        "\n",
        "            reward_steps_hist.append((total_reward, steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JytCmXm-koiT",
        "outputId": "e146cc48-80a4-4f6a-a19d-98f89b038299"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode = 0, Score = -154.52359484408942, Avg_Score = -154.52359484408942\n",
            "(0, -154.52359484408942, 101, 0.8587264897555338)\n",
            "End of episode 0\n",
            "Episode = 1, Score = -250.09701908080118, Avg_Score = -202.31030696244528\n",
            "(1, -250.09701908080118, 86, 0.6083562365039253)\n",
            "End of episode 1\n",
            "Episode = 2, Score = -179.57434281960263, Avg_Score = -194.7316522481644\n",
            "(2, -179.57434281960263, 100, 0.40746640433534903)\n",
            "End of episode 2\n",
            "Episode = 3, Score = -308.11006976672564, Avg_Score = -223.0762566278047\n",
            "(3, -308.11006976672564, 78, 0.298071329000422)\n",
            "End of episode 3\n",
            "Episode = 4, Score = -332.1291216367465, Avg_Score = -244.88682962959305\n",
            "(4, -332.1291216367465, 172, 0.14959821543349452)\n",
            "End of episode 4\n",
            "Episode = 5, Score = -429.81121875145584, Avg_Score = -275.70756114990354\n",
            "(5, -429.81121875145584, 127, 0.08992126184865366)\n",
            "End of episode 5\n",
            "Episode = 6, Score = -233.54829667658763, Avg_Score = -269.68480908228696\n",
            "(6, -233.54829667658763, 138, 0.05171911623156676)\n",
            "End of episode 6\n",
            "Episode = 7, Score = -234.4902421820456, Avg_Score = -265.28548821975676\n",
            "(7, -234.4902421820456, 147, 0.028692860270744993)\n",
            "End of episode 7\n",
            "Episode = 8, Score = -417.2662272070665, Avg_Score = -282.1722369961245\n",
            "(8, -417.2662272070665, 157, 0.01529290447996278)\n",
            "End of episode 8\n",
            "Episode = 9, Score = -525.0733629603983, Avg_Score = -306.4623495925519\n",
            "(9, -525.0733629603983, 100, 0.010242920884816435)\n",
            "End of episode 9\n",
            "Episode = 10, Score = -186.05204845681308, Avg_Score = -295.51595858021204\n",
            "(10, -186.05204845681308, 114, 0.01)\n",
            "End of episode 10\n",
            "Episode = 11, Score = -21.826806365253333, Avg_Score = -272.70852922896546\n",
            "(11, -21.826806365253333, 291, 0.01)\n",
            "End of episode 11\n",
            "Episode = 12, Score = -312.40914087096314, Avg_Score = -275.76242243219605\n",
            "(12, -312.40914087096314, 182, 0.01)\n",
            "End of episode 12\n",
            "Episode = 13, Score = -134.67213917647888, Avg_Score = -265.68454505678767\n",
            "(13, -134.67213917647888, 104, 0.01)\n",
            "End of episode 13\n",
            "Episode = 14, Score = -102.29654151334935, Avg_Score = -254.79201148722512\n",
            "(14, -102.29654151334935, 438, 0.01)\n",
            "End of episode 14\n",
            "Episode = 15, Score = -182.88870525488522, Avg_Score = -250.2980548477039\n",
            "(15, -182.88870525488522, 129, 0.01)\n",
            "End of episode 15\n",
            "Episode = 16, Score = -237.39562574743076, Avg_Score = -249.53908843004078\n",
            "(16, -237.39562574743076, 95, 0.01)\n",
            "End of episode 16\n",
            "Episode = 17, Score = -281.7242953715804, Avg_Score = -251.32715548234853\n",
            "(17, -281.7242953715804, 311, 0.01)\n",
            "End of episode 17\n",
            "Episode = 18, Score = -172.026652062981, Avg_Score = -247.15344477606604\n",
            "(18, -172.026652062981, 93, 0.01)\n",
            "End of episode 18\n",
            "Episode = 19, Score = -93.49114393876104, Avg_Score = -239.4703297342008\n",
            "(19, -93.49114393876104, 71, 0.01)\n",
            "End of episode 19\n",
            "Episode = 20, Score = -235.0422697060917, Avg_Score = -239.25946973286227\n",
            "(20, -235.0422697060917, 269, 0.01)\n",
            "End of episode 20\n",
            "Episode = 21, Score = -146.06475515038386, Avg_Score = -235.0233463427496\n",
            "(21, -146.06475515038386, 136, 0.01)\n",
            "End of episode 21\n",
            "Episode = 22, Score = -124.64019265728696, Avg_Score = -230.22407879120775\n",
            "(22, -124.64019265728696, 55, 0.01)\n",
            "End of episode 22\n",
            "Episode = 23, Score = -171.6825111630618, Avg_Score = -227.78484680670167\n",
            "(23, -171.6825111630618, 192, 0.01)\n",
            "End of episode 23\n",
            "Episode = 24, Score = -136.02821233339364, Avg_Score = -224.11458142776934\n",
            "(24, -136.02821233339364, 74, 0.01)\n",
            "End of episode 24\n",
            "Episode = 25, Score = -152.24156219329592, Avg_Score = -221.35023453413575\n",
            "(25, -152.24156219329592, 209, 0.01)\n",
            "End of episode 25\n",
            "Episode = 26, Score = -344.3319417062015, Avg_Score = -225.90511257754557\n",
            "(26, -344.3319417062015, 224, 0.01)\n",
            "End of episode 26\n",
            "Episode = 27, Score = -215.5770417072054, Avg_Score = -225.53625290360486\n",
            "(27, -215.5770417072054, 332, 0.01)\n",
            "End of episode 27\n",
            "End of episode 28\n",
            "Episode = 29, Score = -364.6640441441183, Avg_Score = -230.3337629463812\n",
            "(29, -364.6640441441183, 1089, 0.01)\n",
            "End of episode 29\n",
            "Episode = 30, Score = -497.11123749950366, Avg_Score = -239.22634543148527\n",
            "(30, -497.11123749950366, 1484, 0.01)\n",
            "End of episode 30\n",
            "Episode = 31, Score = -385.23449710369283, Avg_Score = -243.9362858080081\n",
            "(31, -385.23449710369283, 1300, 0.01)\n",
            "End of episode 31\n",
            "End of episode 32\n",
            "Episode = 33, Score = -12.146072945481237, Avg_Score = -236.69284165605413\n",
            "(33, -12.146072945481237, 307, 0.01)\n",
            "End of episode 33\n",
            "Episode = 34, Score = -349.91985516631365, Avg_Score = -240.12396327757716\n",
            "(34, -349.91985516631365, 1863, 0.01)\n",
            "End of episode 34\n",
            "End of episode 35\n",
            "End of episode 36\n",
            "Episode = 37, Score = -201.5165129072863, Avg_Score = -238.98845003139215\n",
            "(37, -201.5165129072863, 839, 0.01)\n",
            "End of episode 37\n",
            "End of episode 38\n",
            "End of episode 39\n",
            "Episode = 40, Score = -488.81162299909096, Avg_Score = -246.12625497332638\n",
            "(40, -488.81162299909096, 2869, 0.01)\n",
            "End of episode 40\n",
            "Episode = 41, Score = -372.710870424496, Avg_Score = -249.64249429141444\n",
            "(41, -372.710870424496, 370, 0.01)\n",
            "End of episode 41\n",
            "End of episode 42\n",
            "End of episode 43\n",
            "End of episode 44\n",
            "End of episode 45\n",
            "Episode = 46, Score = -394.7483583279165, Avg_Score = -253.5642744005091\n",
            "(46, -394.7483583279165, 276, 0.01)\n",
            "End of episode 46\n",
            "Episode = 47, Score = -357.73157647687157, Avg_Score = -256.30551919199235\n",
            "(47, -357.73157647687157, 1259, 0.01)\n",
            "End of episode 47\n",
            "End of episode 48\n",
            "End of episode 49\n",
            "End of episode 50\n",
            "End of episode 51\n",
            "End of episode 52\n",
            "End of episode 53\n",
            "End of episode 54\n",
            "Episode = 55, Score = -279.32474512451506, Avg_Score = -256.89575575436476\n",
            "(55, -279.32474512451506, 1309, 0.01)\n",
            "End of episode 55\n",
            "Episode = 56, Score = -135.93622805075253, Avg_Score = -253.87176756177436\n",
            "(56, -135.93622805075253, 88, 0.01)\n",
            "End of episode 56\n",
            "Episode = 57, Score = -176.49484639811817, Avg_Score = -251.984525582173\n",
            "(57, -176.49484639811817, 597, 0.01)\n",
            "End of episode 57\n",
            "Episode = 58, Score = -242.94386389498894, Avg_Score = -251.76927173247813\n",
            "(58, -242.94386389498894, 1170, 0.01)\n",
            "End of episode 58\n",
            "Episode = 59, Score = -339.2730663247165, Avg_Score = -253.8042436997395\n",
            "(59, -339.2730663247165, 280, 0.01)\n",
            "End of episode 59\n",
            "Episode = 60, Score = -526.0279447466373, Avg_Score = -259.9911459962599\n",
            "(60, -526.0279447466373, 2254, 0.01)\n",
            "End of episode 60\n",
            "Episode = 61, Score = -213.27348596584613, Avg_Score = -258.9529757733618\n",
            "(61, -213.27348596584613, 467, 0.01)\n",
            "End of episode 61\n",
            "Episode = 62, Score = -116.48022532978455, Avg_Score = -255.85574206806663\n",
            "(62, -116.48022532978455, 374, 0.01)\n",
            "End of episode 62\n",
            "Episode = 63, Score = -245.07324645286747, Avg_Score = -255.62632726774328\n",
            "(63, -245.07324645286747, 502, 0.01)\n",
            "End of episode 63\n",
            "End of episode 64\n",
            "Episode = 65, Score = -118.12079183017204, Avg_Score = -252.76162861279386\n",
            "(65, -118.12079183017204, 1148, 0.01)\n",
            "End of episode 65\n",
            "Episode = 66, Score = -41.98272178822813, Avg_Score = -248.46001826943538\n",
            "(66, -41.98272178822813, 157, 0.01)\n",
            "End of episode 66\n",
            "Episode = 67, Score = -136.83909329079586, Avg_Score = -246.22759976986256\n",
            "(67, -136.83909329079586, 100, 0.01)\n",
            "End of episode 67\n",
            "Episode = 68, Score = -11.56129010429855, Avg_Score = -241.6262995803417\n",
            "(68, -11.56129010429855, 96, 0.01)\n",
            "End of episode 68\n",
            "Episode = 69, Score = -277.6037998852984, Avg_Score = -242.31817458620628\n",
            "(69, -277.6037998852984, 620, 0.01)\n",
            "End of episode 69\n",
            "Episode = 70, Score = 47.60200083314419, Avg_Score = -236.84798259716194\n",
            "(70, 47.60200083314419, 204, 0.01)\n",
            "End of episode 70\n",
            "Episode = 71, Score = -132.1139352410366, Avg_Score = -234.90846320167816\n",
            "(71, -132.1139352410366, 167, 0.01)\n",
            "End of episode 71\n",
            "Episode = 72, Score = -386.37444104764415, Avg_Score = -237.66239007160482\n",
            "(72, -386.37444104764415, 121, 0.01)\n",
            "End of episode 72\n",
            "Episode = 73, Score = -706.0193293729761, Avg_Score = -246.02590684484358\n",
            "(73, -706.0193293729761, 1907, 0.01)\n",
            "End of episode 73\n",
            "Episode = 74, Score = -3.5855259103639128, Avg_Score = -241.77256682844921\n",
            "(74, -3.5855259103639128, 309, 0.01)\n",
            "End of episode 74\n",
            "Episode = 75, Score = -96.60488707488079, Avg_Score = -239.26967579821525\n",
            "(75, -96.60488707488079, 226, 0.01)\n",
            "End of episode 75\n",
            "Episode = 76, Score = -211.5391434116445, Avg_Score = -238.79966677471407\n",
            "(76, -211.5391434116445, 191, 0.01)\n",
            "End of episode 76\n",
            "Episode = 77, Score = -297.93755013641623, Avg_Score = -239.78529816407575\n",
            "(77, -297.93755013641623, 382, 0.01)\n",
            "End of episode 77\n",
            "End of episode 78\n",
            "Episode = 79, Score = -277.70595550199056, Avg_Score = -240.40694828436946\n",
            "(79, -277.70595550199056, 264, 0.01)\n",
            "End of episode 79\n",
            "Episode = 80, Score = -260.6285718423003, Avg_Score = -240.73310350304575\n",
            "(80, -260.6285718423003, 306, 0.01)\n",
            "End of episode 80\n",
            "Episode = 81, Score = -179.1081710148146, Avg_Score = -239.75492997148652\n",
            "(81, -179.1081710148146, 94, 0.01)\n",
            "End of episode 81\n",
            "End of episode 82\n",
            "Episode = 83, Score = -205.5003119269994, Avg_Score = -239.21970156454142\n",
            "(83, -205.5003119269994, 575, 0.01)\n",
            "End of episode 83\n",
            "Episode = 84, Score = -128.01394618895753, Avg_Score = -237.50884378953245\n",
            "(84, -128.01394618895753, 278, 0.01)\n",
            "End of episode 84\n",
            "End of episode 85\n",
            "Episode = 86, Score = -669.8509621003243, Avg_Score = -244.05948194575657\n",
            "(86, -669.8509621003243, 2682, 0.01)\n",
            "End of episode 86\n",
            "End of episode 87\n",
            "End of episode 88\n",
            "End of episode 89\n",
            "Episode = 90, Score = -347.2066480358279, Avg_Score = -245.59899188739942\n",
            "(90, -347.2066480358279, 245, 0.01)\n",
            "End of episode 90\n",
            "End of episode 91\n",
            "End of episode 92\n",
            "End of episode 93\n",
            "End of episode 94\n",
            "End of episode 95\n",
            "Episode = 96, Score = -200.08129682002925, Avg_Score = -244.92961401876164\n",
            "(96, -200.08129682002925, 1023, 0.01)\n",
            "End of episode 96\n",
            "Episode = 97, Score = -242.80962023520647, Avg_Score = -244.89888947117387\n",
            "(97, -242.80962023520647, 1033, 0.01)\n",
            "End of episode 97\n",
            "Episode = 98, Score = -293.0813641512308, Avg_Score = -245.58721053803185\n",
            "(98, -293.0813641512308, 391, 0.01)\n",
            "End of episode 98\n",
            "Episode = 99, Score = -425.2197567735801, Avg_Score = -248.11724640050434\n",
            "(99, -425.2197567735801, 2797, 0.01)\n",
            "End of episode 99\n",
            "Episode = 100, Score = -161.38091112706675, Avg_Score = -246.91257507726212\n",
            "(100, -161.38091112706675, 322, 0.01)\n",
            "End of episode 100\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "gamma = .99\n",
        "alpha = .001\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_decay = .996\n",
        "epsilon_min = .01\n",
        "\n",
        "update_frequency = 10  # in steps\n",
        "memory_size = 1000000\n",
        "num_episodes = 400\n",
        "\n",
        "layers_list = [Dense(64, input_shape=(8,), activation='relu'), Dense(64, activation='relu'), Dense(4, activation='linear')]  # TDL: soft-code inut_shape and num_actions (as part of the class)\n",
        "\n",
        "agent = Agent(env, batch_size, gamma, alpha, epsilon, epsilon_decay, epsilon_min, update_frequency, memory_size)\n",
        "\n",
        "agent.train(num_episodes, layers_list, input_dims=8, loss='mse', optimizer=Adam)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpOSNBFU8FYYxsECKH5fWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}