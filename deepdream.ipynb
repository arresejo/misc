{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arresejo/misc/blob/main/deepdream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sc5Yq_Rgxreb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from keras.models import Sequential, load_model, clone_model\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.losses import Huber\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7NrsKUu6ViB",
        "outputId": "4602bfbf-8d41-4852-e15a-2ad6d30feacf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g_Qp173_NbG5"
      },
      "outputs": [],
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/rogerxcn/lunar_lander_project/blob/master/dqn_agent_keras.py\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.activations import relu, linear\n",
        "#import lunar_lander as lander\n",
        "from collections import deque\n",
        "import gym\n",
        "import random\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(64, input_dim=8, activation=relu))\n",
        "model.add(keras.layers.Dense(64, activation=relu))\n",
        "model.add(keras.layers.Dense(4, activation=linear))\n",
        "# model.compile(loss=\"mse\", optimizer=keras.optimizers.adam(lr=learning_rate ))\n",
        "model.compile(loss=\"mse\", optimizer=Adam(lr=learning_rate))\n",
        "\n",
        "epsilon = 1\n",
        "gamma = .99\n",
        "batch_size = 64\n",
        "memory = deque(maxlen=1000000)\n",
        "min_eps = 0.01\n",
        "model = model\n",
        "\n",
        "def replay_experiences():\n",
        "    if len(memory) >= batch_size:\n",
        "        sample_choices = np.array(memory)\n",
        "        mini_batch_index = np.random.choice(len(sample_choices), batch_size)\n",
        "        #batch = random.sample(memory, batch_size)\n",
        "        states = []\n",
        "        actions = []\n",
        "        next_states = []\n",
        "        rewards = []\n",
        "        finishes = []\n",
        "        for index in mini_batch_index:\n",
        "            states.append(memory[index][0])\n",
        "            actions.append(memory[index][1])\n",
        "            next_states.append(memory[index][2])\n",
        "            rewards.append(memory[index][3])\n",
        "            finishes.append(memory[index][4])\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        next_states = np.array(next_states)\n",
        "        rewards = np.array(rewards)\n",
        "        finishes = np.array(finishes)\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "        q_vals_next_state = model.predict_on_batch(next_states)\n",
        "        q_vals_target = model.predict_on_batch(states)\n",
        "        max_q_values_next_state = np.amax(q_vals_next_state, axis=1)\n",
        "        q_vals_target[np.arange(batch_size), actions] = rewards + gamma * (max_q_values_next_state) * (1 - finishes)\n",
        "        model.fit(states, q_vals_target, verbose=0)\n",
        "        global epsilon\n",
        "        if epsilon > min_eps:\n",
        "            epsilon *= 0.996\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #env = lander.LunarLander()\n",
        "    # env.seed(0)\n",
        "    num_episodes = 400\n",
        "    np.random.seed(0)\n",
        "    scores  = []\n",
        "    for i in range(num_episodes+1):\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "        finished = False\n",
        "        if i != 0 and i % 50 == 0:\n",
        "            model.save(\".\\saved_models\\model_\"+str(i)+\"_episodes.h5\")\n",
        "        for j in range(3000):\n",
        "            state = np.reshape(state, (1, 8))\n",
        "            if np.random.random() <= epsilon:\n",
        "                action =  np.random.choice(4)\n",
        "            else:\n",
        "                action_values = model.predict(state)\n",
        "                action = np.argmax(action_values[0])\n",
        "\n",
        "            #env.render()\n",
        "            next_state, reward, finished, metadata = env.step(action)\n",
        "            next_state = np.reshape(next_state, (1, 8))\n",
        "            memory.append((state, action, next_state, reward, finished))\n",
        "            replay_experiences()\n",
        "            score += reward\n",
        "            state = next_state\n",
        "            if finished:\n",
        "                scores.append(score)\n",
        "                print(\"Episode = {}, Score = {}, Avg_Score = {}\".format(i, score, np.mean(scores[-100:])))\n",
        "                break\n"
      ],
      "metadata": {
        "id": "ofgOzZBTPm26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3719e7-3450-43d6-a523-32283c638a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 0, Score = -106.33521900614146, Avg_Score = -106.33521900614146\n",
            "Episode = 1, Score = -104.79358080494598, Avg_Score = -105.56439990554372\n",
            "Episode = 2, Score = -41.14642312892306, Avg_Score = -84.09174098000351\n",
            "Episode = 3, Score = -167.6868918438088, Avg_Score = -104.99052869595482\n",
            "Episode = 4, Score = -239.60025151031925, Avg_Score = -131.9124732588277\n",
            "Episode = 5, Score = -98.26193079864593, Avg_Score = -126.30404951546409\n",
            "Episode = 6, Score = -164.03931991723636, Avg_Score = -131.69480243000297\n",
            "Episode = 7, Score = 7.07589507365708, Avg_Score = -114.34846524204548\n",
            "Episode = 8, Score = -41.4842301030514, Avg_Score = -106.25243911549059\n",
            "Episode = 9, Score = 36.79992763292415, Avg_Score = -91.94720244064911\n",
            "Episode = 10, Score = -28.21459183408425, Avg_Score = -86.15332874914321\n",
            "Episode = 11, Score = -99.08590750417791, Avg_Score = -87.2310436453961\n",
            "Episode = 12, Score = -88.4108733904923, Avg_Score = -87.32179977963428\n",
            "Episode = 13, Score = -189.79963847066387, Avg_Score = -94.6416454004221\n",
            "Episode = 14, Score = -80.1401946539832, Avg_Score = -93.67488201732617\n",
            "Episode = 15, Score = -170.2806524390226, Avg_Score = -98.4627426686822\n",
            "Episode = 16, Score = -65.9478779858679, Avg_Score = -96.55010356969312\n",
            "Episode = 17, Score = -168.98685157050588, Avg_Score = -100.57436734751606\n",
            "Episode = 18, Score = -77.1047022732959, Avg_Score = -99.33912181729394\n",
            "Episode = 19, Score = -125.94933769925913, Avg_Score = -100.6696326113922\n",
            "Episode = 20, Score = -231.63647618807767, Avg_Score = -106.90614897218674\n",
            "Episode = 21, Score = -19.804872847799984, Avg_Score = -102.9470000574419\n",
            "Episode = 22, Score = -281.9615965993951, Avg_Score = -110.73024338535292\n",
            "Episode = 23, Score = -155.84670565132683, Avg_Score = -112.61009597976847\n",
            "Episode = 24, Score = -143.9480881810313, Avg_Score = -113.86361566781899\n",
            "Episode = 25, Score = -389.67712870545733, Avg_Score = -124.47182770772815\n",
            "Episode = 26, Score = -38.591932538439224, Avg_Score = -121.29109084960635\n",
            "Episode = 27, Score = -410.42351599029945, Avg_Score = -131.6172488903454\n",
            "Episode = 28, Score = -375.0861099528476, Avg_Score = -140.01272685801788\n",
            "Episode = 29, Score = -54.44944784001268, Avg_Score = -137.16061755741768\n",
            "Episode = 30, Score = -48.19923489413302, Avg_Score = -134.29089553602142\n",
            "Episode = 31, Score = -430.4767487518335, Avg_Score = -143.54670344901555\n",
            "Episode = 32, Score = -151.4216118656908, Avg_Score = -143.78533703739964\n",
            "Episode = 33, Score = -259.97036213647016, Avg_Score = -147.20254365796052\n",
            "Episode = 34, Score = -204.4815478821755, Avg_Score = -148.83908663579524\n",
            "Episode = 35, Score = -46.179139381897826, Avg_Score = -145.98742143429808\n",
            "Episode = 36, Score = -247.76915562847816, Avg_Score = -148.73827911522187\n",
            "Episode = 37, Score = -194.44053043048285, Avg_Score = -149.94096993930768\n",
            "Episode = 38, Score = -55.82812455709086, Avg_Score = -147.52782005771238\n",
            "Episode = 39, Score = -85.87837746685942, Avg_Score = -145.9865839929411\n",
            "Episode = 40, Score = -78.53806909733721, Avg_Score = -144.34149826378\n",
            "Episode = 41, Score = -91.00651027283368, Avg_Score = -143.07161759732892\n",
            "Episode = 42, Score = -64.27647606121116, Avg_Score = -141.23917244532618\n",
            "Episode = 43, Score = -120.1462548127434, Avg_Score = -140.75978795367658\n",
            "Episode = 44, Score = -104.87992993887448, Avg_Score = -139.96245777556987\n",
            "Episode = 45, Score = -56.50237367126154, Avg_Score = -138.1481081211284\n",
            "Episode = 46, Score = -156.5020049836669, Avg_Score = -138.53861656501218\n",
            "Episode = 47, Score = -97.60439463569477, Avg_Score = -137.68582027481804\n",
            "Episode = 48, Score = -70.6504007605253, Avg_Score = -136.31775048881207\n",
            "Episode = 49, Score = -139.87899077187524, Avg_Score = -136.38897529447334\n",
            "Episode = 50, Score = -136.64426379033776, Avg_Score = -136.393980951255\n",
            "Episode = 51, Score = 2.374111084571922, Avg_Score = -133.72536379671985\n",
            "Episode = 52, Score = -608.4612798257579, Avg_Score = -142.68264523123\n",
            "Episode = 53, Score = -37.417307188019684, Avg_Score = -140.7332871193187\n",
            "Episode = 54, Score = -539.7999352122434, Avg_Score = -147.98904435737188\n",
            "Episode = 55, Score = -118.35602745613569, Avg_Score = -147.45988334127838\n",
            "Episode = 56, Score = -297.27802437525713, Avg_Score = -150.088271780471\n",
            "Episode = 57, Score = -168.68731480110714, Avg_Score = -150.40894493599922\n",
            "Episode = 58, Score = -283.14492120211133, Avg_Score = -152.65870724559431\n",
            "Episode = 59, Score = -84.13117126254154, Avg_Score = -151.51658164587676\n",
            "Episode = 60, Score = -90.28242879344633, Avg_Score = -150.51274307452542\n",
            "Episode = 61, Score = -79.26775023363233, Avg_Score = -149.36363028676908\n",
            "Episode = 62, Score = -72.94080239761163, Avg_Score = -148.15056952662374\n",
            "Episode = 63, Score = -99.77183723352947, Avg_Score = -147.39465183454416\n",
            "Episode = 64, Score = -0.028059657134022586, Avg_Score = -145.12747349335322\n",
            "Episode = 65, Score = -178.11268296898834, Avg_Score = -145.62724939449922\n",
            "Episode = 66, Score = -52.84767957425489, Avg_Score = -144.2424796956896\n",
            "Episode = 67, Score = -12.715142799492062, Avg_Score = -142.30825415309846\n",
            "Episode = 68, Score = -49.27336358970076, Avg_Score = -140.95992240580284\n",
            "Episode = 69, Score = -250.20744660637268, Avg_Score = -142.52060132295384\n",
            "Episode = 70, Score = -20.218841136447598, Avg_Score = -140.798041320327\n",
            "Episode = 71, Score = -10.056977338064694, Avg_Score = -138.98219320946225\n",
            "Episode = 72, Score = -36.831980104041165, Avg_Score = -137.58287522171676\n",
            "Episode = 73, Score = 5.313294762138018, Avg_Score = -135.6518458976106\n",
            "Episode = 74, Score = -12.70609692415263, Avg_Score = -134.01256924463115\n",
            "Episode = 75, Score = -224.0912685830798, Avg_Score = -135.1978152885581\n",
            "Episode = 76, Score = -70.2907816595643, Avg_Score = -134.35486679986985\n",
            "Episode = 77, Score = -128.4335489735193, Avg_Score = -134.27895246876278\n",
            "Episode = 78, Score = -195.90307047229874, Avg_Score = -135.05900459538984\n",
            "Episode = 79, Score = -180.27768371938615, Avg_Score = -135.6242380844398\n",
            "Episode = 80, Score = -39.074561476774186, Avg_Score = -134.43226676829582\n",
            "Episode = 81, Score = -96.4950191625636, Avg_Score = -133.9696174072503\n",
            "Episode = 82, Score = -56.78657592381643, Avg_Score = -133.03970124479926\n",
            "Episode = 83, Score = -216.54928661625402, Avg_Score = -134.03386297541184\n",
            "Episode = 84, Score = -94.30341503919908, Avg_Score = -133.56644594086816\n",
            "Episode = 85, Score = -33.50529208398859, Avg_Score = -132.40294415183467\n",
            "Episode = 86, Score = -34.01282445548099, Avg_Score = -131.27202323578464\n",
            "Episode = 87, Score = 2.0803402611983826, Avg_Score = -129.75665546877346\n",
            "Episode = 88, Score = -604.0643131425757, Avg_Score = -135.0859549931982\n",
            "Episode = 89, Score = -444.209386733443, Avg_Score = -138.520659790312\n",
            "Episode = 90, Score = -63.917003538141756, Avg_Score = -137.7008393919365\n",
            "Episode = 91, Score = -77.44968770515067, Avg_Score = -137.04593556925406\n",
            "Episode = 92, Score = -448.9531110259274, Avg_Score = -140.39977616556237\n",
            "Episode = 93, Score = -10.619720314517142, Avg_Score = -139.01913727352996\n",
            "Episode = 94, Score = -72.88078649771106, Avg_Score = -138.32294410746871\n",
            "Episode = 95, Score = -47.28883383888722, Avg_Score = -137.37467212550436\n",
            "Episode = 96, Score = -134.06110898003737, Avg_Score = -137.34051168070573\n",
            "Episode = 97, Score = -270.230172064292, Avg_Score = -138.69652862339538\n",
            "Episode = 98, Score = -82.12922388261686, Avg_Score = -138.12514170682186\n",
            "Episode = 99, Score = -222.15247598746168, Avg_Score = -138.96541504962823\n",
            "Episode = 100, Score = -14.390994794495647, Avg_Score = -138.0459728075118\n",
            "Episode = 101, Score = -183.68983476690434, Avg_Score = -138.83493534713136\n",
            "Episode = 102, Score = -20.700885472642746, Avg_Score = -138.6304799705686\n",
            "Episode = 103, Score = -68.26529117422803, Avg_Score = -137.63626396387278\n",
            "Episode = 104, Score = -36.724956242866135, Avg_Score = -135.60751101119826\n",
            "Episode = 105, Score = -85.15816848440029, Avg_Score = -135.47647338805578\n",
            "Episode = 106, Score = -23.7935184055505, Avg_Score = -134.07401537293893\n",
            "Episode = 107, Score = -58.18242816303781, Avg_Score = -134.72659860530587\n",
            "Episode = 108, Score = -19.048650432765555, Avg_Score = -134.50224280860303\n",
            "Episode = 109, Score = -8.233947240465184, Avg_Score = -134.9525815573369\n",
            "Episode = 110, Score = -49.14503190717224, Avg_Score = -135.16188595806778\n",
            "Episode = 111, Score = -40.069121015344116, Avg_Score = -134.57171809317944\n",
            "Episode = 112, Score = -92.36158989635432, Avg_Score = -134.61122525823805\n",
            "Episode = 113, Score = -63.04409845989088, Avg_Score = -133.34366985813034\n",
            "Episode = 114, Score = -114.6678163770125, Avg_Score = -133.68894607536063\n",
            "Episode = 115, Score = -53.333062957640685, Avg_Score = -132.51947018054682\n",
            "Episode = 116, Score = -2.4691965545540775, Avg_Score = -131.88468336623367\n",
            "Episode = 117, Score = -1.089487221157828, Avg_Score = -130.20570972274018\n",
            "Episode = 118, Score = -26.0812528232063, Avg_Score = -129.69547522823927\n",
            "Episode = 119, Score = -216.34673332020418, Avg_Score = -130.59944918444873\n",
            "Episode = 120, Score = -61.200379138341425, Avg_Score = -128.89508821395137\n",
            "Episode = 121, Score = -15.43006995169756, Avg_Score = -128.85134018499036\n",
            "Episode = 122, Score = -36.491011654075, Avg_Score = -126.39663433553714\n",
            "Episode = 123, Score = -44.14712226927569, Avg_Score = -125.27963850171665\n",
            "Episode = 124, Score = -7.494383404999193, Avg_Score = -123.91510145395632\n",
            "Episode = 125, Score = -42.2842878077199, Avg_Score = -120.44117304497894\n",
            "Episode = 126, Score = -48.10250593579782, Avg_Score = -120.53627877895252\n",
            "Episode = 127, Score = -73.48768439858902, Avg_Score = -117.16692046303544\n",
            "Episode = 128, Score = 3.571413156834466, Avg_Score = -113.3803452319386\n",
            "Episode = 129, Score = -63.927765023584115, Avg_Score = -113.47512840377435\n",
            "Episode = 130, Score = -176.19457921329143, Avg_Score = -114.75508184696591\n",
            "Episode = 131, Score = -276.72905629490424, Avg_Score = -113.21760492239662\n",
            "Episode = 132, Score = -56.106242496709285, Avg_Score = -112.2644512287068\n",
            "Episode = 133, Score = -19.105234469533308, Avg_Score = -109.85579995203743\n",
            "Episode = 134, Score = -64.52097534648858, Avg_Score = -108.45619422668057\n",
            "Episode = 135, Score = -79.5707525425575, Avg_Score = -108.79011035828715\n",
            "Episode = 136, Score = -175.86562404066004, Avg_Score = -108.07107504240898\n",
            "Episode = 137, Score = -60.72108603339444, Avg_Score = -106.73388059843808\n",
            "Episode = 138, Score = -2.1966086473144735, Avg_Score = -106.19756543934032\n",
            "Episode = 139, Score = -34.64619278020767, Avg_Score = -105.68524359247384\n",
            "Episode = 140, Score = -15.66065344977147, Avg_Score = -105.05646943599817\n",
            "Episode = 141, Score = -39.49659509569036, Avg_Score = -104.54137028422674\n",
            "Episode = 142, Score = 4.666509734803102, Avg_Score = -103.8519404262666\n",
            "Episode = 143, Score = -133.97479574569383, Avg_Score = -103.99022583559608\n",
            "Episode = 144, Score = -302.3885954150426, Avg_Score = -105.96531249035777\n",
            "Episode = 145, Score = -1.5947668448165624, Avg_Score = -105.41623642209332\n",
            "Episode = 146, Score = -61.153843567130906, Avg_Score = -104.46275480792796\n",
            "Episode = 147, Score = -15.524782031339708, Avg_Score = -103.64195868188443\n",
            "Episode = 148, Score = -7.794116412439242, Avg_Score = -103.01339583840354\n",
            "Episode = 149, Score = 14.25273410939444, Avg_Score = -101.47207858959084\n",
            "Episode = 150, Score = -66.80730848449016, Avg_Score = -100.77370903653238\n",
            "Episode = 151, Score = -31.225987016798104, Avg_Score = -101.10971001754609\n",
            "Episode = 152, Score = -8.619304970915369, Avg_Score = -95.11129026899765\n",
            "Episode = 153, Score = -77.38382453005585, Avg_Score = -95.510955442418\n",
            "Episode = 154, Score = 7.03100556974446, Avg_Score = -90.04264603459814\n",
            "Episode = 155, Score = -20.766236438302812, Avg_Score = -89.0667481244198\n",
            "Episode = 156, Score = -26.50688358093192, Avg_Score = -86.35903671647657\n",
            "Episode = 157, Score = -193.43504413995902, Avg_Score = -86.60651400986508\n",
            "Episode = 158, Score = -27.68561701208265, Avg_Score = -84.0519209679648\n",
            "Episode = 159, Score = -14.945187654268642, Avg_Score = -83.36006113188205\n",
            "Episode = 160, Score = -54.49011631366833, Avg_Score = -83.00213800708428\n",
            "Episode = 161, Score = -37.373687595149455, Avg_Score = -82.58319738069946\n",
            "Episode = 162, Score = -577.8961375989317, Avg_Score = -87.63275073271265\n",
            "Episode = 163, Score = -9.845745380187081, Avg_Score = -86.73348981417922\n",
            "Episode = 164, Score = -5.928226759184499, Avg_Score = -86.79249148519972\n",
            "Episode = 165, Score = -38.6747130017433, Avg_Score = -85.39811178552728\n",
            "Episode = 166, Score = -8.273459772011298, Avg_Score = -84.95236958750485\n",
            "Episode = 167, Score = -33.52505693963837, Avg_Score = -85.16046872890631\n",
            "Episode = 168, Score = -29.196147705877262, Avg_Score = -84.95969657006806\n",
            "Episode = 169, Score = -31.592048979876274, Avg_Score = -82.7735425938031\n",
            "Episode = 170, Score = -22.035878370296736, Avg_Score = -82.79171296614159\n",
            "Episode = 171, Score = -32.8164836581867, Avg_Score = -83.01930802934281\n",
            "Episode = 172, Score = -88.9449152082339, Avg_Score = -83.54043738038473\n",
            "Episode = 173, Score = -60.252350799429855, Avg_Score = -84.19609383600044\n",
            "Episode = 174, Score = -104.78095452842835, Avg_Score = -85.11684241204321\n",
            "Episode = 175, Score = -32.26454120486344, Avg_Score = -83.19857513826103\n",
            "Episode = 176, Score = -167.0045509005186, Avg_Score = -84.16571283067059\n",
            "Episode = 177, Score = -6.815267160018752, Avg_Score = -82.94953001253556\n",
            "Episode = 178, Score = -180.1032038421422, Avg_Score = -82.79153134623401\n",
            "Episode = 179, Score = -73.82401619521981, Avg_Score = -81.72699467099234\n",
            "Episode = 180, Score = -116.74435746452356, Avg_Score = -82.50369263086982\n",
            "Episode = 181, Score = -40.448440369605684, Avg_Score = -81.94322684294026\n",
            "Episode = 182, Score = -27.194096260816927, Avg_Score = -81.64730204631026\n",
            "Episode = 183, Score = -88.42076115037338, Avg_Score = -80.36601679165146\n",
            "Episode = 184, Score = -160.82997093152122, Avg_Score = -81.03128235057467\n",
            "Episode = 185, Score = -49.44364678852973, Avg_Score = -81.1906658976201\n",
            "Episode = 186, Score = -38.336199688921575, Avg_Score = -81.2338996499545\n",
            "Episode = 187, Score = -73.82986044645212, Avg_Score = -81.99300165703099\n",
            "Episode = 188, Score = -11.843894095729855, Avg_Score = -76.07079746656251\n",
            "Episode = 189, Score = -37.79155287322087, Avg_Score = -72.0066191279603\n",
            "Episode = 190, Score = -67.57698862368922, Avg_Score = -72.04321897881577\n",
            "Episode = 191, Score = -17.390301997362258, Avg_Score = -71.44262512173789\n",
            "Episode = 192, Score = -217.66215985702533, Avg_Score = -69.12971561004889\n",
            "Episode = 193, Score = -215.4672789865975, Avg_Score = -71.17819119676969\n",
            "Episode = 194, Score = -42.384544459536464, Avg_Score = -70.87322877638793\n",
            "Episode = 195, Score = -74.73196168338576, Avg_Score = -71.14766005483291\n",
            "Episode = 196, Score = -32.434317368327875, Avg_Score = -70.13139213871582\n",
            "Episode = 197, Score = -165.05025825943164, Avg_Score = -69.07959300066723\n",
            "Episode = 198, Score = -79.59510206857557, Avg_Score = -69.05425178252682\n",
            "Episode = 199, Score = -174.3735057385135, Avg_Score = -68.57646208003733\n",
            "Episode = 200, Score = 100.76543223282374, Avg_Score = -67.42489780976413\n",
            "Episode = 201, Score = -25.48207162062165, Avg_Score = -65.8428201783013\n",
            "Episode = 202, Score = -49.87480521193901, Avg_Score = -66.13455937569428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, env, batch_size, gamma, alpha, epsilon, epsilon_decay, epsilon_min, update_frequency, model_fname, memory_size=1000000):\n",
        "    self.env = env\n",
        "    if not self.env.continuous:  # discrete\n",
        "      self.num_actions = 4  # line 170 in source code: https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
        "    else:\n",
        "      return f\"you still did not work on coninuous...\"\n",
        "    self.memory_size = 50000\n",
        "    memory_space = 8+1+1+8+1  # states, action, reward, next_state, terminal   # TDL (soft-code it from the env)\n",
        "    self.memory_counter = 0\n",
        "    self.memory = np.zeros((memory_space, self.memory_size))\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "    self.alpha = alpha\n",
        "    # self.num_episodes = num_episodes  # in the train method\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.update_frequency = update_frequency\n",
        "    self.model_fname = model_fname\n",
        "\n",
        "  def agent_model(self, layer_list, input_dims, lr, loss, optimizer):  # fcn\n",
        "    model = Sequential()\n",
        "    # for l_name, l_size, l_activation in layer_list[1:]:\n",
        "    #   model.add(l_name(l_size, activation=l_activation))\n",
        "    for l in layer_list:\n",
        "      model.add(l)\n",
        "    model.compile(optimizer=optimizer(learning_rate=lr), loss=loss)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "  \n",
        "  # def model_fit(self):\n",
        "  #   pass\n",
        "\n",
        "  def store_in_memory(self, sarst_list):  # sarst_list := (state,action,next_state,reward,terminal) \n",
        "    # store transition (S, A, R, S', terminal) in replay memory D\n",
        "    self.memory[:,self.memory_counter%self.memory_size] = np.concatenate(tuple(np.array(x,dtype=float,ndmin=1) for x in sarst_list))\n",
        "    self.memory_counter += 1\n",
        "  \n",
        "  def choose_action(self, state):\n",
        "    if np.random.random() > self.epsilon:\n",
        "      action = np.argmax(self.model_q1.predict(state.reshape((1,8))))  # epsilon-greedy\n",
        "    else:\n",
        "      action = np.random.randint(self.num_actions)\n",
        "    return action\n",
        "\n",
        "  def get_rand_sample(self, memo):\n",
        "    # return memo[:,np.random.choice(memo.shape[1], size=self.batch_size, replace=False)]\n",
        "    # return memo[:,np.random.choice(memo.shape[0], size=self.batch_size, replace=False)]\n",
        "    # print(memo.shape)\n",
        "    # print(memo.shape[0], self.batch_size)\n",
        "\n",
        "    out = memo[:,np.random.choice(memo.shape[1], size=self.batch_size, replace=False)]\n",
        "    # print(out.shape)\n",
        "    return out\n",
        "  \n",
        "  def train(self, num_episodes, layer_list, input_dims=8, loss='mse', optimizer=Adam):  # TDL: default of input_dims as num of states\n",
        "    self.model_q1 = self.agent_model(layer_list, input_dims, self.alpha, loss, optimizer)\n",
        "\n",
        "    self.model_q2 = clone_model(self.model_q1)\n",
        "    self.model_q2.set_weights(self.model_q1.get_weights())\n",
        "    reward_steps_hist = []  # tupple per episode:  (reward, num_steps)\n",
        "\n",
        "    for episode in range(num_episodes+1):\n",
        "      total_reward = 0\n",
        "      steps = 0\n",
        "      #terminal = False\n",
        "      state = self.env.reset()  # seed=seed\n",
        "      #while not terminal:\n",
        "      for _ in range(3000):\n",
        "        action = self.choose_action(state)\n",
        "        next_state, reward, terminal, _ = self.env.step(action)\n",
        "        ls=[state, action, reward,next_state,terminal]\n",
        "        self.store_in_memory(ls)\n",
        "        total_reward += reward\n",
        "        if self.memory_counter > self.batch_size:\n",
        "\n",
        "          batch = self.get_rand_sample(self.memory[:,:np.min((self.memory_counter, self.memory_size))])\n",
        "          # y_state = batch[0:8, :]\n",
        "          prediction_q = self.model_q1.predict(batch[0:8, :].T)  # predict the state from batch\n",
        "\n",
        "          batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "          prediction_q[batch_index, np.array(batch[8,:], dtype=np.int32)] = \\\n",
        "              batch[9,:] + (batch[-1,:]==0)*self.gamma*np.max(self.model_q2.predict(batch[10:18,:].T),axis=1)  # batch[-1,:]==0 if terminal  (if terminal else from the pseudocode)\n",
        "          self.model_q1.fit(batch[0:8,:].T, prediction_q, verbose=0)\n",
        "          if steps%self.update_frequency==0:\n",
        "            self.model_q2.set_weights(self.model_q1.get_weights())\n",
        "        state = next_state\n",
        "        # self.epsilon = np.max(((self.epsilon * self.epsilon_decay), self.epsilon_min))\n",
        "\n",
        "        # optional: decay learning rate\n",
        "        \n",
        "        steps += 1\n",
        "      if episode % 10 == 0:\n",
        "        self.save_model()\n",
        "      self.epsilon = np.max(((self.epsilon * self.epsilon_decay), self.epsilon_min))\n",
        "      print((episode, total_reward, steps, self.epsilon))  \n",
        "\n",
        "      reward_steps_hist.append((total_reward, steps))\n",
        "\n",
        "    \n",
        "  def save_model(self):\n",
        "    self.model_q2.save(self.model_fname+\".h5\")\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L6rhRxE7Qm-g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "gamma = .99\n",
        "alpha = .001\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_decay = .996\n",
        "epsilon_min = .01\n",
        "\n",
        "update_frequency = 10  # in steps\n",
        "memory_size = 1000000\n",
        "num_episodes = 400\n",
        "\n",
        "layers_list = [Dense(64, input_shape=(8,), activation='relu'), Dense(64, activation='relu'), Dense(4, activation='linear')]  # TDL: soft-code inut_shape and num_actions (as part of the class)\n",
        "\n",
        "model_fname = 'DQN'\n",
        "agent1 = Agent(env, batch_size, gamma, alpha, epsilon, epsilon_decay, epsilon_min, update_frequency, model_fname, memory_size=1000000)\n",
        "agent1.train(num_episodes, layers_list, input_dims=8, loss=Huber(delta=1.0), optimizer=Adam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "6lrKmdFEaedK",
        "outputId": "2d81a763-a256-4216-fc8e-6e27004d700d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 64)                576       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,996\n",
            "Trainable params: 4,996\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "(0, -290888.99820025836, 3000, 0.996)\n",
            "(1, -292819.23856116843, 3000, 0.992016)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2f227915e482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DQN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0magent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHuber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-cd76abe31a4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_episodes, layer_list, input_dims, loss, optimizer)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m           \u001b[0mprediction_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m               \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_q2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch[-1,:]==0 if terminal  (if terminal else from the pseudocode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_q1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_frequency\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1959\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     self._configure_dataset_and_inferred_steps(strategy, x, steps_per_epoch,\n\u001b[0;32m-> 1170\u001b[0;31m                                                class_weight, distribute)\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m   def _configure_dataset_and_inferred_steps(self, strategy, x, steps_per_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_configure_dataset_and_inferred_steps\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1184\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_validate_data_handler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1328\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_validate_data_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;31m# TODO(b/152094471): Support this with DistIter.get_next_as_optional.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m     if self._steps_per_execution.numpy().item(\n\u001b[0m\u001b[1;32m   1331\u001b[0m     ) > 1 and self._inferred_steps is None:\n\u001b[1;32m   1332\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4064\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 4065\u001b[0;31m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[1;32m   4066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deepdream.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}